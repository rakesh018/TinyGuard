# ğŸ”’ TinyGuard

ğŸ›¡ï¸ **Ultra-light (0.05KB) CNN model for detecting compromised IoT devices**, distilled for edge efficiency using Knowledge Distillation.

---

## ğŸš€ Overview

**TinyGuard** is a hyper-optimized deep learning model designed to run on ultra-resource-constrained IoT environments. Leveraging **Knowledge Distillation**, we compress a powerful CNN into a **tiny 0.05KB model**â€”without sacrificing its ability to detect compromised or anomalous device behavior.

This project showcases how *security* and *efficiency* can co-exist on the edge.

---
## ğŸ”„ Workflow

![TinyGuard Flowchart](./Flowchart.png)

---

## What is Knowledge Distillation?

**Knowledge Distillation** is a model compression technique where a small "student" model is trained to mimic the behavior of a large "teacher" model. Instead of learning from raw labels, the student learns from the teacherâ€™s **soft predictions**â€”capturing richer patterns and decision boundaries.

This technique enables us to:
- Retain much of the performance of large models
- Deploy models in extremely constrained environments (like IoT, microcontrollers)
- Improve inference time, energy consumption, and memory footprint

ğŸ“– **Read more**: [Distilling the Knowledge in a Neural Network (Hinton et al., 2015)](https://arxiv.org/abs/1503.02531)

---



## ğŸ“ Project Structure

* [TinyGuard](./)
  * [Base-Papers](./Base-Papers)  
    * [base_paper_2.pdf](./Base-Papers/base_paper_2.pdf) â€“ Paper on energy use in on-device ML for IoT intrusion detection
  * [Report-PPT](./Report-PPT) â€“ Final project report and presentation
  * [results](./results) â€“ Output graphs, logs, and final results
  * [README.md](README.md) â€“ This file
